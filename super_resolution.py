import warnings
import torch.nn as nn
import torch.nn.functional as F
import numbers
import math
from einops import rearrange
import sys
import os
from PyQt5.QtWidgets import (
    QApplication, QWidget, QPushButton, QLabel, QFileDialog,
    QHBoxLayout, QVBoxLayout, QComboBox, QMessageBox, QListWidget, QListWidgetItem,
    QSizePolicy, QScrollArea, QProgressBar
)
from PyQt5.QtGui import QPixmap, QImage, QFont, QPainter, QColor
from PyQt5.QtCore import Qt, QThread, pyqtSignal, QRect

import torch
from PIL import Image
import torchvision.transforms as transforms
import numpy as np
from ui_core import strip_inline_styles
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            'mean is more than 2 std from [a, b] in nn.init.trunc_normal_. '
            'The distribution of values may be incorrect.',
            stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        low = norm_cdf((a - mean) / std)
        up = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [low, up], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * low - 1, 2 * up - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution.

    From: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/weight_init.py

    The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value

    Examples:
        # >>> w = torch.empty(3, 5)
        # >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)
def img2windows(img, H_sp, W_sp):
    """
    Input: Image (B, C, H, W)
    Output: Window Partition (B', N, C)
    """
    B, C, H, W = img.shape
    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)
    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp * W_sp, C)

    return img_perm


def windows2img(img_splits_hw, H_sp, W_sp, H, W):
    """
    Input: Window Partition (B', N, C)
    Output: Image (B, H, W, C)
    """
    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))

    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)
    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return img


class Mlp(nn.Module):

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.hidden_dims = hidden_features
        self.in_dims = in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)

    def forward(self, x):
        _, n, _ = x.shape
        self.N = n
        x = self.fc1(x)
        x = self.act(x)
        x = self.fc2(x)
        return x


class DynamicPosBias(nn.Module):
    # The implementation builds on Crossformer code https://github.com/cheerss/CrossFormer/blob/main/models/crossformer.py
    """ Dynamic Relative Position Bias.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        residual (bool):  If True, use residual strage to connect conv.
    """

    def __init__(self, dim, num_heads, residual):
        super().__init__()
        self.residual = residual
        self.num_heads = num_heads
        self.pos_dim = dim // 4
        self.pos_proj = nn.Linear(2, self.pos_dim)
        self.pos1 = nn.Sequential(
            nn.LayerNorm(self.pos_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.pos_dim, self.pos_dim),
        )
        self.pos2 = nn.Sequential(
            nn.LayerNorm(self.pos_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.pos_dim, self.pos_dim)
        )
        self.pos3 = nn.Sequential(
            nn.LayerNorm(self.pos_dim),
            nn.ReLU(inplace=True),
            nn.Linear(self.pos_dim, self.num_heads)
        )

    def forward(self, biases):
        self.l, self.c = biases.shape
        if self.residual:
            pos = self.pos_proj(biases)  # 2Gh-1 * 2Gw-1, heads
            pos = pos + self.pos1(pos)
            pos = pos + self.pos2(pos)
            pos = self.pos3(pos)
        else:
            pos = self.pos3(self.pos2(self.pos1(self.pos_proj(biases))))
        return pos


class Attention_regular(nn.Module):
    """ Regular Rectangle-Window (regular-Rwin) self-attention with dynamic relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        dim (int): Number of input channels.
        resolution (int): Input resolution.
        idx (int): The identix of V-Rwin and H-Rwin, 0 is H-Rwin, 1 is Vs-Rwin. (different order from Attention_axial)
        split_size (tuple(int)): Height and Width of the regular rectangle window (regular-Rwin).
        dim_out (int | None): The dimension of the attention output. Default: None
        num_heads (int): Number of attention heads. Default: 6
        qk_scale (float | None): Override default qk scale of head_dim ** -0.5 if set
        position_bias (bool): The dynamic relative position bias. Default: True
    """

    def __init__(self, dim, idx, split_size=[2, 4], dim_out=None, num_heads=6, qk_scale=None, position_bias=True):
        super().__init__()
        self.dim = dim
        self.dim_out = dim_out or dim
        self.split_size = split_size
        self.num_heads = num_heads
        self.idx = idx
        self.position_bias = position_bias

        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        if idx == 0:
            H_sp, W_sp = self.split_size[0], self.split_size[1]
        elif idx == 1:
            W_sp, H_sp = self.split_size[0], self.split_size[1]
        else:
            print("ERROR MODE", idx)
            exit(0)
        self.H_sp = H_sp
        self.W_sp = W_sp

        self.pos = DynamicPosBias(self.dim // 4, self.num_heads, residual=False)
        self.softmax = nn.Softmax(dim=-1)

    def im2win(self, x, H, W):
        B, N, C = x.shape
        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)
        x = img2windows(x, self.H_sp, self.W_sp)
        x = x.reshape(-1, self.H_sp * self.W_sp, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3).contiguous()
        return x

    def forward(self, qkv, H, W, mask=None, rpi=None, rpe_biases=None):
        """
        Input: qkv: (B, 3*L, C), H, W, mask: (B, N, N), N is the window size
        Output: x (B, H, W, C)
        """
        q, k, v = qkv[0], qkv[1], qkv[2]

        B, L, C = q.shape
        assert L == H * W, "flatten img_tokens has wrong size"

        self.N = L // (self.H_sp * self.W_sp)
        # partition the q,k,v, image to window
        q = self.im2win(q, H, W)
        k = self.im2win(k, H, W)
        v = self.im2win(v, H, W)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))  # B head N C @ B head C N --> B head N N

        # calculate drpe
        pos = self.pos(rpe_biases)
        # select position bias
        relative_position_bias = pos[rpi.view(-1)].view(
            self.H_sp * self.W_sp, self.H_sp * self.W_sp, -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()
        attn = attn + relative_position_bias.unsqueeze(0)

        N = attn.shape[3]

        # use mask for shift window
        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
        attn = self.softmax(attn)

        x = (attn @ v)
        x = x.transpose(1, 2).reshape(-1, self.H_sp * self.W_sp, C)  # B head N N @ B head N C

        # merge the window, window to image
        x = windows2img(x, self.H_sp, self.W_sp, H, W)  # B H' W' C

        return x


class SRWAB(nn.Module):
    r""" Shift Rectangle Window Attention Block.

    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        split_size (int): Define the window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self,
                 dim,
                 num_heads,
                 split_size=(2, 2),
                 shift_size=(0, 0),
                 mlp_ratio=2.,
                 qkv_bias=True,
                 qk_scale=None,
                 act_layer=nn.GELU,
                 norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        self.norm1 = norm_layer(dim)
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim)
        self.branch_num = 2
        self.get_v = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim)  # DW Conv

        self.attns = nn.ModuleList([
            Attention_regular(
                dim // 2, idx=i,
                split_size=split_size, num_heads=num_heads // 2, dim_out=dim // 2,
                qk_scale=qk_scale, position_bias=True)
            for i in range(self.branch_num)])

        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)

    def forward(self, x, x_size, params, attn_mask=NotImplementedError):
        h, w = x_size
        self.h, self.w = x_size

        b, l, c = x.shape
        shortcut = x
        x = self.norm1(x)
        qkv = self.qkv(x).reshape(b, -1, 3, c).permute(2, 0, 1, 3)  # 3, B, HW, C
        v = qkv[2].transpose(-2, -1).contiguous().view(b, c, h, w)

        # cyclic shift
        if self.shift_size[0] > 0 or self.shift_size[1] > 0:
            qkv = qkv.view(3, b, h, w, c)
            # H-Shift
            qkv_0 = torch.roll(qkv[:, :, :, :, :c // 2], shifts=(-self.shift_size[0], -self.shift_size[1]), dims=(2, 3))
            qkv_0 = qkv_0.view(3, b, h * w, c // 2)
            # V-Shift
            qkv_1 = torch.roll(qkv[:, :, :, :, c // 2:], shifts=(-self.shift_size[1], -self.shift_size[0]), dims=(2, 3))
            qkv_1 = qkv_1.view(3, b, h * w, c // 2)

            # H-Rwin
            x1_shift = self.attns[0](qkv_0, h, w, mask=attn_mask[0], rpi=params['rpi_sa_h'],
                                     rpe_biases=params['biases_h'])
            # V-Rwin
            x2_shift = self.attns[1](qkv_1, h, w, mask=attn_mask[1], rpi=params['rpi_sa_v'],
                                     rpe_biases=params['biases_v'])

            x1 = torch.roll(x1_shift, shifts=(self.shift_size[0], self.shift_size[1]), dims=(1, 2))
            x2 = torch.roll(x2_shift, shifts=(self.shift_size[1], self.shift_size[0]), dims=(1, 2))
            # Concat
            attened_x = torch.cat([x1, x2], dim=-1)
        else:
            # H-Rwin
            x1 = self.attns[0](qkv[:, :, :, :c // 2], h, w, rpi=params['rpi_sa_h'], rpe_biases=params['biases_h'])
            # V-Rwin
            x2 = self.attns[1](qkv[:, :, :, c // 2:], h, w, rpi=params['rpi_sa_v'], rpe_biases=params['biases_v'])
            # Concat
            attened_x = torch.cat([x1, x2], dim=-1)

        attened_x = attened_x.view(b, -1, c).contiguous()

        # Locality Complementary Module
        lcm = self.get_v(v)
        lcm = lcm.permute(0, 2, 3, 1).contiguous().view(b, -1, c)

        attened_x = attened_x + lcm

        attened_x = self.proj(attened_x)

        # FFN
        x = shortcut + attened_x
        x = x + self.mlp(self.norm2(x))
        return x


class HFERB(nn.Module):
    def __init__(self, dim) -> None:
        super().__init__()
        self.mid_dim = dim // 2
        self.dim = dim
        self.act = nn.GELU()
        self.last_fc = nn.Conv2d(self.dim, self.dim, 1)

        # High-frequency enhancement branch
        self.fc = nn.Conv2d(self.mid_dim, self.mid_dim, 1)
        self.max_pool = nn.MaxPool2d(3, 1, 1)

        # Local feature extraction branch
        self.conv = nn.Conv2d(self.mid_dim, self.mid_dim, 3, 1, 1)

    def forward(self, x):
        self.h, self.w = x.shape[2:]
        short = x

        # Local feature extraction branch
        lfe = self.act(self.conv(x[:, :self.mid_dim, :, :]))

        # High-frequency enhancement branch
        hfe = self.act(self.fc(self.max_pool(x[:, self.mid_dim:, :, :])))

        x = torch.cat([lfe, hfe], dim=1)
        x = short + self.last_fc(x)
        return x


##########################################################################
## High-frequency prior query inter attention layer
class Attention(nn.Module):
    def __init__(self, dim, num_heads, bias, train_size=(1, 3, 48, 48), base_size=(int(48 * 1.5), int(48 * 1.5))):
        super(Attention, self).__init__()
        self.num_heads = num_heads
        self.train_size = train_size
        self.base_size = base_size
        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))
        self.dim = dim
        self.softmax = nn.Softmax(dim=-1)

        self.q = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)
        self.q_dwconv = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim, bias=bias)
        self.kv = nn.Conv2d(dim, dim * 2, kernel_size=1, bias=bias)
        self.kv_dwconv = nn.Conv2d(dim * 2, dim * 2, kernel_size=3, stride=1, padding=1, groups=dim * 2, bias=bias)
        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)

    def _forward(self, q, kv):
        k, v = kv.chunk(2, dim=1)
        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)
        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)

        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)

        attn = (q @ k.transpose(-2, -1)) * self.temperature
        attn = self.softmax(attn)
        out = (attn @ v)
        return out

    def forward(self, low, high):
        self.h, self.w = low.shape[2:]

        q = self.q_dwconv(self.q(high))
        kv = self.kv_dwconv(self.kv(low))
        out = self._forward(q, kv)
        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=kv.shape[-2], w=kv.shape[-1])
        out = self.project_out(out)
        return out


def to_3d(x):
    return rearrange(x, 'b c h w -> b (h w) c')


def to_4d(x, h, w):
    return rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)


class BiasFree_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(BiasFree_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)

        assert len(normalized_shape) == 1

        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return x / torch.sqrt(sigma + 1e-5) * self.weight


class WithBias_LayerNorm(nn.Module):
    def __init__(self, normalized_shape):
        super(WithBias_LayerNorm, self).__init__()
        if isinstance(normalized_shape, numbers.Integral):
            normalized_shape = (normalized_shape,)
        normalized_shape = torch.Size(normalized_shape)

        assert len(normalized_shape) == 1

        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))
        self.normalized_shape = normalized_shape

    def forward(self, x):
        mu = x.mean(-1, keepdim=True)
        sigma = x.var(-1, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5) * self.weight + self.bias


class LayerNorm(nn.Module):
    def __init__(self, dim, LayerNorm_type):
        super(LayerNorm, self).__init__()
        if LayerNorm_type == 'BiasFree':
            self.body = BiasFree_LayerNorm(dim)
        else:
            self.body = WithBias_LayerNorm(dim)

    def forward(self, x):
        h, w = x.shape[-2:]
        return to_4d(self.body(to_3d(x)), h, w)


##########################################################################
## Improved feed-forward network
class FeedForward(nn.Module):
    def __init__(self, dim, ffn_expansion_factor, bias):
        super(FeedForward, self).__init__()

        hidden_features = int(dim * ffn_expansion_factor)
        self.hid_fea = hidden_features
        self.dim = dim

        self.project_in = nn.Conv2d(dim, hidden_features * 2, kernel_size=1, bias=bias)

        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, kernel_size=3, stride=1, padding=1,
                                groups=hidden_features * 2, bias=bias)

        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)

    def forward(self, x):
        self.h, self.w = x.shape[2:]
        x = self.project_in(x)
        x1, x2 = self.dwconv(x).chunk(2, dim=1)
        x = F.gelu(x1) * x2
        x = self.project_out(x)
        return x


##########################################################################
class HFB(nn.Module):
    r""" Hybrid Fusion Block.

    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        ffn_expansion_factor (int): Define the window size.
        bias (int): Shift size for SW-MSA.
        LayerNorm_type (float): Ratio of mlp hidden dim to embedding dim.
    """

    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):
        super(HFB, self).__init__()

        self.norm1 = LayerNorm(dim, LayerNorm_type)
        self.attn = Attention(dim, num_heads, bias)
        self.norm2 = LayerNorm(dim, LayerNorm_type)
        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)
        self.dim = dim

    def forward(self, low, high):
        self.h, self.w = low.shape[2:]
        x = low + self.attn(self.norm1(low), high)
        x = x + self.ffn(self.norm2(x))

        return x


class CRFB(nn.Module):
    """ Cross-Refinement Fusion Block.

    Args:
        dim (int): Number of input channels.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
    """

    def __init__(self,
                 dim,
                 depth,
                 num_heads,
                 split_size_0=7,
                 split_size_1=7,
                 mlp_ratio=2.,
                 qkv_bias=True,
                 qk_scale=None,
                 norm_layer=nn.LayerNorm
                 ):
        super().__init__()
        self.depth = depth

        # Shift Rectangle window attention blocks
        self.srwa_blocks = nn.ModuleList([
            SRWAB(
                dim=dim,
                num_heads=num_heads,
                split_size=[split_size_0, split_size_1],
                shift_size=[0, 0] if (i % 2 == 0) else [split_size_0 // 2, split_size_1 // 2],
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                norm_layer=norm_layer) for i in range(2 * depth)
        ])

        # High frequency enhancement residual blocks
        self.hfer_blocks = nn.ModuleList([
            HFERB(dim)
            for _ in range(depth)])

        # Hybrid fusion blocks
        self.hf_blocks = nn.ModuleList([
            HFB(
                dim=dim,
                num_heads=num_heads,
                ffn_expansion_factor=2.66,
                bias=False,
                LayerNorm_type='WithBias') for i in range(depth)
        ])

    def forward(self, x, x_size, params):
        b, c, h, w = x.shape
        for i in range(self.depth):
            low = x.permute(0, 2, 3, 1)
            low = low.reshape(b, h * w, c)
            low = self.srwa_blocks[2 * i + 1](self.srwa_blocks[2 * i](low, x_size, params, params['attn_mask']), x_size,
                                              params, params['attn_mask'])
            low = low.reshape(b, h, w, c)
            low = low.permute(0, 3, 1, 2)
            high = self.hfer_blocks[i](x)
            x = self.hf_blocks[i](low, high)
        return x


class RCRFG(nn.Module):
    """Residual Cross-Refinement Fusion Group (RCRFG).

    Args:
        dim (int): Number of input channels.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        resi_connection: The convolutional block before residual connection.
    """

    def __init__(self,
                 dim,
                 depth,
                 num_heads,
                 mlp_ratio=2.,
                 qkv_bias=True,
                 qk_scale=None,
                 split_size_0=2,
                 split_size_1=2,
                 norm_layer=nn.LayerNorm
                 ):
        super(RCRFG, self).__init__()

        self.dim = dim

        self.residual_group = CRFB(
            dim=dim,
            depth=depth,
            num_heads=num_heads,
            mlp_ratio=mlp_ratio,
            qkv_bias=qkv_bias,
            qk_scale=qk_scale,
            split_size_0=split_size_0,
            split_size_1=split_size_1,
            norm_layer=norm_layer
        )

        self.conv = nn.Conv2d(dim, dim, 3, 1, 1)

    def forward(self, x, x_size, params):
        self.h, self.w = x_size
        return self.conv(self.residual_group(x, x_size, params)) + x


class UpsampleOneStep(nn.Sequential):
    """UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)
       Used in lightweight SR to save parameters.

    Args:
        scale (int): Scale factor. Supported scales: 2^n and 3.
        num_feat (int): Channel number of intermediate features.

    """

    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):
        self.num_feat = num_feat
        self.input_resolution = input_resolution
        self.scale = scale
        m = []
        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))
        m.append(nn.PixelShuffle(scale))
        super(UpsampleOneStep, self).__init__(*m)


class CRAFT(nn.Module):
    r""" Cross-Refinement Adaptive Fusion Transformer
        Some codes are based on SwinIR.
    Args:
        in_chans (int): Number of input image channels. Default: 3
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 2
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        upscale: Upscale factor. 2/3/4/
        img_range: Image range. 1. or 255.
        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'
    """

    def __init__(self,
                 in_chans=3,
                 embed_dim=96,
                 depths=(6, 6, 6, 6),
                 num_heads=(6, 6, 6, 6),
                 split_size_0=4,
                 split_size_1=16,
                 mlp_ratio=2.,
                 qkv_bias=True,
                 qk_scale=None,
                 norm_layer=nn.LayerNorm,
                 upscale=2,
                 img_range=1.,
                 upsampler='',
                 resi_connection='1conv',
                 **kwargs):
        super(CRAFT, self).__init__()

        self.split_size = (split_size_0, split_size_1)

        num_in_ch = in_chans
        num_out_ch = in_chans
        num_feat = 64
        self.img_range = img_range
        self.num_feat = num_feat
        self.num_out_ch = num_out_ch
        if in_chans == 3:
            rgb_mean = (0.4488, 0.4371, 0.4040)
            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)
        else:
            self.mean = torch.zeros(1, 1, 1, 1)
        self.upscale = upscale
        self.upsampler = upsampler

        # relative position index
        self.calculate_rpi_v_sa()

        # ------------------------- 1, shallow feature extraction ------------------------- #
        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)

        # ------------------------- 2, deep feature extraction ------------------------- #
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.num_features = embed_dim
        self.mlp_ratio = mlp_ratio

        # build Residual Cross-Refinement Fusion Group
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = RCRFG(
                dim=embed_dim,
                depth=depths[i_layer],
                num_heads=num_heads[i_layer],
                mlp_ratio=self.mlp_ratio,
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                split_size_0=split_size_0,
                split_size_1=split_size_1,
                norm_layer=norm_layer
            )
            self.layers.append(layer)

        self.norm = LayerNorm(self.num_features, 'with_bias')

        # build the last conv layer in deep feature extraction
        if resi_connection == '1conv':
            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)
        elif resi_connection == 'identity':
            self.conv_after_body = nn.Identity()

        # ------------------------- 3, high quality image reconstruction ------------------------- #
        self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def calculate_rpi_v_sa(self):
        # generate mother-set
        H_sp, W_sp = self.split_size[0], self.split_size[1]
        position_bias_h = torch.arange(1 - H_sp, H_sp)
        position_bias_w = torch.arange(1 - W_sp, W_sp)
        biases_h = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))
        biases_h = biases_h.flatten(1).transpose(0, 1).contiguous().float()

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(H_sp)
        coords_w = torch.arange(W_sp)
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += H_sp - 1
        relative_coords[:, :, 1] += W_sp - 1
        relative_coords[:, :, 0] *= 2 * W_sp - 1
        relative_position_index_h = relative_coords.sum(-1)

        H_sp, W_sp = self.split_size[1], self.split_size[0]
        position_bias_h = torch.arange(1 - H_sp, H_sp)
        position_bias_w = torch.arange(1 - W_sp, W_sp)
        biases_v = torch.stack(torch.meshgrid([position_bias_h, position_bias_w]))
        biases_v = biases_v.flatten(1).transpose(0, 1).contiguous().float()

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(H_sp)
        coords_w = torch.arange(W_sp)
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += H_sp - 1
        relative_coords[:, :, 1] += W_sp - 1
        relative_coords[:, :, 0] *= 2 * W_sp - 1
        relative_position_index_v = relative_coords.sum(-1)
        self.register_buffer('relative_position_index_h', relative_position_index_h)
        self.register_buffer('relative_position_index_v', relative_position_index_v)
        self.register_buffer('biases_v', biases_v)
        self.register_buffer('biases_h', biases_h)

        return biases_v, biases_h

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        x_size = (x.shape[2], x.shape[3])
        params = {'attn_mask': (None, None), 'rpi_sa_h': self.relative_position_index_h,
                  'rpi_sa_v': self.relative_position_index_v, 'biases_v': self.biases_v, 'biases_h': self.biases_h}

        for layer in self.layers:
            x = layer(x, x_size, params)

        x = self.norm(x)

        return x

    def forward(self, x):
        self.h, self.w = x.shape[2:]
        self.mean = self.mean.type_as(x)
        x = (x - self.mean) * self.img_range

        x = self.conv_first(x)
        x = self.conv_after_body(self.forward_features(x)) + x

        x = self.upsample(x)
        x = x / self.img_range + self.mean
        return x

# ---------------------- QT 应用代码 ----------------------
class ModelWrapper:
    def __init__(self, model_path, scale, device='cuda'):
        self.scale = scale
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"Initializing CRAFT model with scale={scale}, device={self.device}")

        # 初始化 CRAFT 模型，参数与测试代码一致
        self.model = CRAFT(
            upscale=scale,
            in_chans=3,
            embed_dim=48,
            depths=(2, 2, 2, 2),
            num_heads=(6, 6, 6, 6),
            mlp_ratio=2,
            resi_connection='1conv',
            norm_layer=nn.LayerNorm
        ).to(self.device)

        # 加载模型权重
        try:
            loadnet = torch.load(model_path, map_location=self.device)
            print(f"Loaded model keys: {list(loadnet.keys())}")  # 调试信息
            if 'params_ema' in loadnet:
                keyname = 'params_ema'
            else:
                keyname = 'params'
            self.model.load_state_dict(loadnet[keyname], strict=True)
            print("Model state_dict loaded successfully.")
        except Exception as e:
            print(f"Error loading state_dict: {e}")
            raise e

        self.model.eval()

    def preprocess(self, image):
        transform = transforms.ToTensor()
        image = transform(image).unsqueeze(0).to(self.device)
        return image

    def postprocess(self, tensor):
        tensor = tensor.squeeze().cpu().detach().numpy()
        tensor = np.transpose(tensor, (1, 2, 0))
        tensor = np.clip(tensor * 255.0, 0, 255).astype(np.uint8)
        image = Image.fromarray(tensor)
        return image

    def infer(self, image):
        try:
            with torch.no_grad():
                input_tensor = self.preprocess(image)
                # 填充输入图像，使其尺寸为 window_size 的倍数
                window_size = 16
                _, _, h_old, w_old = input_tensor.size()
                h_pad = (h_old // window_size + 1) * window_size - h_old
                w_pad = (w_old // window_size + 1) * window_size - w_old
                input_tensor = torch.nn.functional.pad(input_tensor, (0, w_pad, 0, h_pad), mode='reflect')
                print(f"Padded input tensor to size: {input_tensor.size()}")

                output_tensor = self.model(input_tensor)
                output_tensor = output_tensor[..., :h_old * self.scale, :w_old * self.scale]
                output_image = self.postprocess(output_tensor)
            return output_image
        except Exception as e:
            print(f"Error during inference: {e}")
            raise e

class InferenceThread(QThread):
    result_ready = pyqtSignal(dict)  # 输出格式: dict {image_name: output_image}
    error_signal = pyqtSignal(str)
    progress_signal = pyqtSignal(int)

    def __init__(self, models, images, result_dir):
        super().__init__()
        self.models = models  # 字典，包含倍率和对应模型
        self.images = images  # 列表，包含 (scale, image_path, image_name) 元组
        self.result_dir = result_dir

    def run(self):
        try:
            output_images = {}
            total = len(self.images)
            for idx, (scale, image_path, image_name) in enumerate(self.images):
                progress = int(((idx + 1) / total) * 100)
                self.progress_signal.emit(progress)
                model = self.models.get(scale)
                if model is None:
                    raise ValueError(f"未加载{scale}倍模型。")
                print(f"Inference on image {idx + 1}/{total}: {image_name} with scale {scale}x")
                image = Image.open(image_path).convert('RGB')
                output_image = model.infer(image)
                # 不再自动保存图片到 result 目录
                # output_path = os.path.join(self.result_dir, f"{image_name}_CRAFT.png")
                # output_image.save(output_path)
                output_images[image_name] = output_image
                # print(f"Saved reconstructed image to: {output_path}")
            self.result_ready.emit(output_images)
        except Exception as e:
            self.error_signal.emit(str(e))

    def display_selected_image(self, item):
        try:
            name_scale = item.text()
            if '(' in name_scale and ')' in name_scale:
                image_name = name_scale.split('(')[0].strip()
                scale_str = name_scale.split('(')[1].strip('x)')
                scale = int(scale_str)
            else:
                image_name = name_scale
                scale = 1  # 默认倍率

            self.current_selection = image_name  # 设置当前选中的图像名称

            # 获取图像在列表中的索引
            idx = self.image_list.row(item)
            scale, image_path, name = self.images[idx]

            # 显示原始图像
            try:
                image = Image.open(image_path).convert('RGB')
                self.display_image(self.original_label, self.original_size_label, image, is_original=True)
                self.original_title.setText(f'原始图像 ({scale}x)')
            except Exception as e:
                print(f"Error loading image for display: {e}")
                QMessageBox.critical(self, '错误', f"显示图像失败: {e}")

            # 如果存在重建后的图像，则显示
            if image_name in self.output_images:
                self.display_reconstructed_image(image_name)
            else:
                # 清空重建后图像区域
                self.output_label.clear()
                self.output_size_label.clear()
                self.output_title.setText('重建后图像')

        except Exception as e:
            print(f"Error parsing item text: {e}")
            QMessageBox.critical(self, '错误', f"解析图像信息失败: {e}")

class DeletableListWidget(QListWidget):
    delete_requested = pyqtSignal(QListWidgetItem)  # 自定义信号，当删除请求时发射

    def __init__(self, parent=None):
        super().__init__(parent)

    def keyPressEvent(self, event):
        if event.key() == Qt.Key_Delete:
            selected_items = self.selectedItems()
            if selected_items:
                for item in selected_items:
                    self.delete_requested.emit(item)
        else:
            super().keyPressEvent(event)

class SuperResolutionApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle('细胞图像超分辨率重建')
        self.setAcceptDrops(True)  # 允许拖放
        self.models = {}  # 存储不同倍率的模型
        self.init_ui()
        strip_inline_styles(self)

    def init_ui(self):
        # 按钮字体设置
        button_font = QFont()
        button_font.setPointSize(12)  # 增大字体大小

        # 按钮
        self.load_model_btn = QPushButton('加载模型')
        self.load_image_btn = QPushButton('选择图像')
        self.run_btn = QPushButton('开始重建')
        self.save_btn = QPushButton('保存结果')

        # 设置按钮字体和尺寸
        for btn in [self.load_model_btn, self.load_image_btn, self.run_btn, self.save_btn]:
            btn.setFont(button_font)
            btn.setFixedHeight(50)  # 设置固定高度
            btn.setFixedWidth(180)   # 设置固定宽度

        self.run_btn.setEnabled(False)
        self.save_btn.setEnabled(False)

        # 倍率选择
        self.scale_combo = QComboBox()
        self.scale_combo.setFont(button_font)
        self.scale_combo.addItems(['2倍', '4倍'])
        self.scale_combo.setFixedHeight(50)
        self.scale_combo.setFixedWidth(100)

        # 图像列表显示（使用 DeletableListWidget）
        self.image_list = DeletableListWidget()
        self.image_list.setFixedWidth(300)
        self.image_list.setStyleSheet("background-color: #FFFFFF;")
        self.image_list.setFont(button_font)
        self.image_list.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Expanding)

        # 连接删除信号
        self.image_list.delete_requested.connect(self.delete_selected_image)

        # 图像显示区域的标题
        self.original_title = QLabel('原始图像')
        self.original_title.setAlignment(Qt.AlignCenter)
        self.original_title.setFont(QFont('', 12, QFont.Bold))

        self.output_title = QLabel('重建后图像')
        self.output_title.setAlignment(Qt.AlignCenter)
        self.output_title.setFont(QFont('', 12, QFont.Bold))

        # 图像显示区域
        # 原始图像显示
        self.original_label = QLabel()
        self.original_label.setObjectName("original_label")  # 设置对象名称
        self.original_label.setAlignment(Qt.AlignCenter)
        self.original_label.setStyleSheet("border: 2px solid #CCCCCC; background-color: #F8F8F8;")
        self.original_label.setSizePolicy(QSizePolicy.Ignored, QSizePolicy.Ignored)
        self.original_label.setScaledContents(False)  # 不拉伸图像

        self.original_size_label = QLabel()
        self.original_size_label.setAlignment(Qt.AlignLeft | Qt.AlignTop)
        self.original_size_label.setFont(QFont('', 10))
        self.original_size_label.setStyleSheet("color: black;")

        # 重建后图像显示
        self.output_label = QLabel()
        self.output_label.setObjectName("output_label")  # 设置对象名称
        self.output_label.setAlignment(Qt.AlignCenter)
        self.output_label.setStyleSheet("border: 2px solid #CCCCCC; background-color: #F8F8F8;")
        self.output_label.setSizePolicy(QSizePolicy.Ignored, QSizePolicy.Ignored)
        self.output_label.setScaledContents(False)  # 不拉伸图像

        self.output_size_label = QLabel()
        self.output_size_label.setAlignment(Qt.AlignLeft | Qt.AlignTop)
        self.output_size_label.setFont(QFont('', 10))
        self.output_size_label.setStyleSheet("color: black;")

        # 使用滚动区域确保图像显示适应窗口大小
        original_scroll = QScrollArea()
        original_scroll.setWidgetResizable(True)  # 设置为 True，允许根据内容调整大小
        original_scroll.setWidget(self.original_label)

        output_scroll = QScrollArea()
        output_scroll.setWidgetResizable(True)  # 设置为 True，允许根据内容调整大小
        output_scroll.setWidget(self.output_label)

        # 图像部分的布局
        original_layout = QVBoxLayout()
        original_layout.addWidget(self.original_title)
        original_layout.addWidget(original_scroll)
        original_layout.addWidget(self.original_size_label, alignment=Qt.AlignLeft)

        output_layout = QVBoxLayout()
        output_layout.addWidget(self.output_title)
        output_layout.addWidget(output_scroll)
        output_layout.addWidget(self.output_size_label, alignment=Qt.AlignLeft)

        images_layout = QHBoxLayout()
        images_layout.addLayout(original_layout)
        images_layout.addLayout(output_layout)

        # 进度条
        self.progress_bar = QProgressBar()
        self.progress_bar.setValue(0)
        self.progress_bar.setVisible(False)
        self.progress_bar.setFixedHeight(30)
        self.progress_bar.setFormat("超分辨率重建进度: %p%")
        self.progress_bar.setAlignment(Qt.AlignCenter)
        self.progress_bar.setStyleSheet("""
            QProgressBar {
                border: 2px solid #CCCCCC;
                border-radius: 5px;
                text-align: center;
                background-color: #FFFFFF;
            }
            QProgressBar::chunk {
                background-color: #4CAF50;
                width: 20px;
            }
        """)

        # 按钮布局
        buttons_layout = QHBoxLayout()
        buttons_layout.addWidget(self.load_model_btn)
        buttons_layout.addWidget(self.load_image_btn)

        # 进度条布局（放在加载模型和倍率选择之间）
        progress_layout = QHBoxLayout()
        progress_layout.addWidget(self.progress_bar)

        # 倍率选择布局
        scale_layout = QHBoxLayout()
        scale_label = QLabel('倍率选择:')
        scale_label.setFont(button_font)  # 设置字体
        scale_label.setAlignment(Qt.AlignVCenter | Qt.AlignRight)
        scale_layout.addWidget(scale_label)
        scale_layout.addWidget(self.scale_combo)
        scale_layout.addWidget(self.run_btn)
        scale_layout.addWidget(self.save_btn)

        # 按钮和倍率选择的组合布局
        top_layout = QVBoxLayout()
        top_layout.addLayout(buttons_layout)
        top_layout.addLayout(progress_layout)  # 将进度条添加到按钮和倍率选择之间
        top_layout.addLayout(scale_layout)

        # 图像和列表布局
        main_images_layout = QHBoxLayout()
        main_images_layout.addWidget(self.image_list)
        main_images_layout.addLayout(images_layout)

        # 主布局
        main_layout = QVBoxLayout()
        main_layout.addLayout(top_layout)
        main_layout.addLayout(main_images_layout)

        self.setLayout(main_layout)

        # 应用样式表以美化界面
        self.setStyleSheet("""
            QWidget {
                background-color: #F0F0F0;
                color: #000000;
            }
            QPushButton {
                background-color: #4CAF50;
                color: white;
                border: none;
                border-radius: 10px;
            }
            QPushButton:hover {
                background-color: #45a049;
            }
            QPushButton:disabled {
                background-color: #A5D6A7;
            }
            QComboBox {
                background-color: #FFFFFF;
                color: #000000;
                border-radius: 5px;
                padding: 5px;
            }
            QLabel {
                color: #000000;
                font-weight: bold;
            }
            QListWidget {
                border: 2px solid #CCCCCC;
                border-radius: 5px;
                background-color: #FFFFFF;
            }
        """)

        # 连接信号
        self.load_model_btn.clicked.connect(self.load_model)
        self.load_image_btn.clicked.connect(self.load_image)
        self.run_btn.clicked.connect(self.run_inference)
        self.save_btn.clicked.connect(self.save_image)
        self.image_list.itemClicked.connect(self.display_selected_image)

        # 状态
        self.images = []  # 列表，存储 (scale, image_path, image_name) 元组
        self.output_images = {}  # 字典，存储 image_name: output_image
        self.current_selection = None  # 当前选中的图像名称

        # 设置结果文件夹路径
        self.result_dir = 'result'
        os.makedirs(self.result_dir, exist_ok=True)

    def display_image(self, label, size_label, image, is_original=True):
        try:
            qimage = self.pil_to_qimage(image)
            if qimage.isNull():
                print("转换后的 QImage 为空。")
                QMessageBox.critical(self, '错误', '转换后的 QImage 为空。')
                return

            pixmap = QPixmap.fromImage(qimage)

            # 设置 QPixmap 到 QLabel
            label.setPixmap(pixmap)
            # 不调整 QLabel 大小，因为 QScrollArea 已处理

            # 设置尺寸信息
            width, height = image.size
            image_size = f"{width}x{height}"
            size_label.setText(f"尺寸: {image_size}")

            print(f"图像已成功显示在 {label.objectName()} 上，尺寸为 {pixmap.size().width()}x{pixmap.size().height()}")
        except Exception as e:
            print(f"Error displaying image: {e}")
            QMessageBox.critical(self, '错误', f"显示图像失败: {e}")

    def pil_to_qimage(self, image):
        image = image.convert('RGB')
        width, height = image.size
        data = image.tobytes('raw', 'RGB')
        bytes_per_line = width * 3
        qimage = QImage(data, width, height, bytes_per_line, QImage.Format_RGB888).copy()
        if qimage.isNull():
            print("转换后的 QImage 为空。")
        return qimage

    def load_model(self):
        # 获取当前选择的倍率
        scale_text = self.scale_combo.currentText()
        scale = 2 if scale_text == '2倍' else 4
        print(f"Selected scale for model loading: {scale}x")

        # 弹出文件对话框选择模型文件
        model_path, _ = QFileDialog.getOpenFileName(
            self, f'选择{scale}倍模型文件', './model', 'PyTorch模型 (*.pth *.pt)')
        if model_path:
            try:
                print(f"Loading model from: {model_path}")
                model = ModelWrapper(model_path, scale=scale)
                self.models[scale] = model
                QMessageBox.information(self, '提示', f'{scale}倍模型加载成功！')

                # 更新按钮文本
                self.load_model_btn.setText(f'{scale}倍模型已加载')

                # 检查是否至少有一个模型被加载
                if self.models:
                    self.run_btn.setEnabled(True)
            except Exception as e:
                print(f"Error loading model: {e}")
                QMessageBox.critical(self, '错误', f"加载模型失败: {e}")

    def load_image(self):
        options = QFileDialog.Options()
        image_paths, _ = QFileDialog.getOpenFileNames(
            self, '选择图像文件', './data', '图像文件 (*.png *.jpg *.jpeg *.bmp)', options=options)
        if image_paths:
            # 清空当前批次的图片和结果
            self.images = []
            self.output_images = {}
            self.current_selection = None
            self.image_list.clear()
            self.original_label.clear()
            self.original_size_label.clear()
            self.original_title.setText('原始图像')
            self.output_label.clear()
            self.output_size_label.clear()
            self.output_title.setText('重建后图像')
            for path in image_paths:
                try:
                    print(f"Loading image: {path}")
                    image_name = os.path.splitext(os.path.basename(path))[0]
                    scale = 2 if self.scale_combo.currentText() == '2倍' else 4
                    self.images.append((scale, path, image_name))
                    item = QListWidgetItem(f"{image_name} ({scale}x)")
                    self.image_list.addItem(item)
                except Exception as e:
                    print(f"Error loading image {path}: {e}")
                    QMessageBox.critical(self, '错误', f"加载图像失败: {path}\n错误信息: {e}")
            QMessageBox.information(self, '提示', '图像加载完成！')
            # 自动显示第一张图片
            if len(self.images) > 0:
                self.image_list.setCurrentRow(0)
                self.display_selected_image(self.image_list.item(0))

    def run_inference(self):
        if not self.images:
            QMessageBox.warning(self, '警告', '请先加载一张或多张图像。')
            return
        if not self.models:
            QMessageBox.warning(self, '警告', '请先加载至少一个模型。')
            return

        self.run_btn.setEnabled(False)
        self.save_btn.setEnabled(False)
        self.progress_bar.setValue(0)
        self.progress_bar.setVisible(True)
        self.progress_bar.setFormat('超分辨率重建进度: %p%')

        # 不再清空result目录
        # if os.path.exists(self.result_dir):
        #     shutil.rmtree(self.result_dir)
        # os.makedirs(self.result_dir, exist_ok=True)
        print(f"Result directory: {self.result_dir}")

        # 执行推理
        try:
            self.thread = InferenceThread(self.models, self.images, self.result_dir)
            self.thread.result_ready.connect(self.on_inference_finished)
            self.thread.error_signal.connect(self.on_inference_error)
            self.thread.progress_signal.connect(self.update_progress)
            self.thread.start()
        except Exception as e:
            print(f"Error during inference setup: {e}")
            QMessageBox.critical(self, '错误', f"图像重建失败: {e}")
            self.progress_bar.setValue(0)
            self.progress_bar.setVisible(False)
            self.run_btn.setEnabled(True)
            self.save_btn.setEnabled(True)

    def on_inference_finished(self, output_images):
        self.output_images = output_images
        self.progress_bar.setFormat('重建完成！')
        self.progress_bar.setVisible(False)
        self.run_btn.setEnabled(True)
        self.save_btn.setEnabled(True)
        QMessageBox.information(self, '提示', '图像重建完成！')
        # 自动刷新当前选中的图像显示
        if self.current_selection:
            if self.current_selection in self.output_images:
                self.display_reconstructed_image(self.current_selection)

    def on_inference_error(self, error_message):
        QMessageBox.critical(self, '错误', f"图像重建失败: {error_message}")
        self.progress_bar.setValue(0)
        self.progress_bar.setVisible(False)
        self.run_btn.setEnabled(True)
        self.save_btn.setEnabled(True)

    def update_progress(self, progress):
        self.progress_bar.setValue(progress)

    def save_image(self):
        if not self.output_images:
            QMessageBox.warning(self, '警告', '没有重建后的图像可保存。')
            return
        import datetime
        batch_time = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        save_root = os.path.join(os.getcwd(), "result", "super_resolution", batch_time)
        os.makedirs(save_root, exist_ok=True)
        save_dir = QFileDialog.getExistingDirectory(self, '选择保存目录', save_root)
        if save_dir:
            try:
                # 只保存当前批次的推理结果
                for image_name, output_image in self.output_images.items():
                    base_name = f"{image_name}_CRAFT"
                    image_path = os.path.join(save_dir, f"{base_name}.png")
                    count = 1
                    while os.path.exists(image_path):
                        image_path = os.path.join(save_dir, f"{base_name}_{count}.png")
                        count += 1
                    output_image.save(image_path)
                    print(f"Saved {image_name} to {image_path}")
                QMessageBox.information(self, '提示', f"所有图像已保存到 {save_dir}")
            except Exception as e:
                print(f"Error saving images: {e}")
                QMessageBox.critical(self, '错误', f"保存图像失败: {e}")

    def display_selected_image(self, item):
        try:
            name_scale = item.text()
            if '(' in name_scale and ')' in name_scale:
                image_name = name_scale.split('(')[0].strip()
                scale_str = name_scale.split('(')[1].strip('x)')
                scale = int(scale_str)
            else:
                image_name = name_scale
                scale = 1  # 默认倍率

            self.current_selection = image_name  # 设置当前选中的图像名称

            # 获取图像在列表中的索引
            idx = self.image_list.row(item)
            scale, image_path, name = self.images[idx]

            # 显示原始图像
            try:
                image = Image.open(image_path).convert('RGB')
                self.display_image(self.original_label, self.original_size_label, image, is_original=True)
                self.original_title.setText(f'原始图像 ({scale}x)')
            except Exception as e:
                print(f"Error loading image for display: {e}")
                QMessageBox.critical(self, '错误', f"显示图像失败: {e}")

            # 如果存在重建后的图像，则显示
            if image_name in self.output_images:
                self.display_reconstructed_image(image_name)
            else:
                # 清空重建后图像区域
                self.output_label.clear()
                self.output_size_label.clear()
                self.output_title.setText('重建后图像')

        except Exception as e:
            print(f"Error parsing item text: {e}")
            QMessageBox.critical(self, '错误', f"解析图像信息失败: {e}")

    def display_reconstructed_image(self, image_name):
        output_image = self.output_images.get(image_name)
        if output_image:
            # 获取对应的倍率
            scale = self.get_image_scale(image_name)
            self.display_image(self.output_label, self.output_size_label, output_image, is_original=False)
            self.output_title.setText(f'重建后图像 ({scale}x)')

    def get_image_scale(self, image_name):
        for scale, image_path, name in self.images:
            if name == image_name:
                return scale
        return 1

    def delete_selected_image(self, item):
        try:
            # 获取图像名称和倍率
            name_scale = item.text()
            if '(' in name_scale and ')' in name_scale:
                image_name = name_scale.split('(')[0].strip()
                scale_str = name_scale.split('(')[1].strip('x)')
                scale = int(scale_str)
            else:
                image_name = name_scale
                scale = 1  # 默认倍率

            # 获取图像在列表中的索引
            idx = self.image_list.row(item)

            # 从数据结构中移除
            if idx >= 0 and idx < len(self.images):
                del self.images[idx]

            # 从输出图像中移除（如果存在）
            if image_name in self.output_images:
                del self.output_images[image_name]
                # 删除对应的文件
                output_path = os.path.join(self.result_dir, f"{image_name}_CRAFT.png")
                if os.path.exists(output_path):
                    os.remove(output_path)
                    print(f"Deleted reconstructed image file: {output_path}")

            # 如果删除的是当前选中的图像，清空显示
            if self.current_selection == image_name:
                self.original_label.clear()
                self.original_size_label.clear()
                self.original_title.setText('原始图像')
                self.output_label.clear()
                self.output_size_label.clear()
                self.output_title.setText('重建后图像')
                self.current_selection = None

            # 从列表中删除
            self.image_list.takeItem(idx)
            print(f"Deleted image: {image_name} ({scale}x)")
        except Exception as e:
            print(f"Error deleting image: {e}")
            QMessageBox.critical(self, '错误', f"删除图像失败: {e}")

    def dragEnterEvent(self, event):
        if event.mimeData().hasUrls():
            event.acceptProposedAction()

    def dropEvent(self, event):
        urls = event.mimeData().urls()
        image_paths = [url.toLocalFile() for url in urls if url.isLocalFile()]
        if image_paths:
            for path in image_paths:
                try:
                    print(f"Dropping image: {path}")
                    # 只记录路径，不在此处加载图像
                    image_name = os.path.splitext(os.path.basename(path))[0]
                    scale = 2 if self.scale_combo.currentText() == '2倍' else 4
                    self.images.append((scale, path, image_name))
                    # 添加到列表
                    item = QListWidgetItem(f"{image_name} ({scale}x)")
                    self.image_list.addItem(item)
                except Exception as e:
                    print(f"Error dropping image {path}: {e}")
                    QMessageBox.critical(self, '错误', f"加载图像失败: {path}\n错误信息: {e}")
            QMessageBox.information(self, '提示', '图像加载完成！')
            # 如果只有一张图，自动选择
            if len(self.images) == 1:
                self.image_list.setCurrentRow(0)
                self.display_selected_image(self.image_list.item(0))


if __name__ == '__main__':
    app = QApplication(sys.argv)
    window = SuperResolutionApp()
    window.resize(1600, 900)  # 调整窗口大小
    window.show()
    sys.exit(app.exec_())